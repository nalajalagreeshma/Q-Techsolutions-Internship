{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalajalagreeshma/Q-Techsolutions-Internship/blob/main/video_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFnJsE6vjf8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install yolov5\n",
        "!pip install opencv-python opencv-python-headless"
      ],
      "metadata": {
        "id": "b5knUyTGx3Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from yolov5 import YOLOv5\n",
        "\n",
        "# Initialize YOLOv5 model with COCO-trained weights (yolov5s.pt)\n",
        "model = YOLOv5('yolov5s.pt', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize Video Capture\n",
        "video_path = '/content/4261446-uhd_3840_2160_25fps.mp4'  # Replace with your video file or use 0 for webcam\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if the video capture is opened correctly\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# ----- Video Stabilization Preparation -----\n",
        "# Read the first frame\n",
        "_, prev_frame = cap.read()\n",
        "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Initialize ORB detector for finding keypoints and descriptors\n",
        "orb = cv2.ORB_create()\n",
        "\n",
        "# Get keypoints and descriptors for the first frame\n",
        "kp1, des1 = orb.detectAndCompute(prev_gray, None)\n",
        "\n",
        "# Define transformation matrix storage\n",
        "transforms = []\n",
        "\n",
        "# ----- Motion Tracking Preparation -----\n",
        "# Parameters for Lucas-Kanade optical flow\n",
        "lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
        "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "# Points for motion tracking\n",
        "old_gray = prev_gray.copy()\n",
        "p0 = cv2.goodFeaturesToTrack(old_gray, maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
        "\n",
        "# Loop through each frame in the video\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"End of video reached or no frames captured.\")\n",
        "        break\n",
        "\n",
        "    # ----- Video Stabilization -----\n",
        "    # Convert current frame to grayscale\n",
        "    curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Find keypoints and descriptors for the current frame\n",
        "    kp2, des2 = orb.detectAndCompute(curr_gray, None)\n",
        "\n",
        "    # Match descriptors between frames using BFMatcher\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    matches = bf.match(des1, des2)\n",
        "\n",
        "    # Sort matches based on distance (lower distance is better)\n",
        "    matches = sorted(matches, key=lambda x: x.distance)\n",
        "\n",
        "    # Extract the matched keypoints\n",
        "    pts_prev = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    pts_curr = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "\n",
        "    # Find the affine transformation matrix based on matched points\n",
        "    transform, _ = cv2.estimateAffinePartial2D(pts_prev, pts_curr)\n",
        "\n",
        "    # Store the transformation\n",
        "    transforms.append(transform)\n",
        "\n",
        "    # Apply the transformation to stabilize the frame\n",
        "    frame_stabilized = cv2.warpAffine(frame, transform, (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "    # Update previous keypoints and descriptors for the next iteration\n",
        "    prev_gray = curr_gray.copy()\n",
        "    kp1, des1 = kp2, des2\n",
        "\n",
        "    # ----- Motion Tracking -----\n",
        "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, curr_gray, p0, None, **lk_params)\n",
        "\n",
        "    # Select good points\n",
        "    good_new = p1[st == 1]\n",
        "    good_old = p0[st == 1]\n",
        "\n",
        "    # Draw the tracks for motion tracking\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        a, b = new.ravel()\n",
        "        c, d = old.ravel()\n",
        "\n",
        "        # Convert float points to integer points\n",
        "        a, b, c, d = int(a), int(b), int(c), int(d)\n",
        "\n",
        "        # Draw the line representing motion tracking\n",
        "        frame_stabilized = cv2.line(frame_stabilized, (a, b), (c, d), (0, 255, 0), 2)\n",
        "\n",
        "    old_gray = curr_gray.copy()\n",
        "    p0 = good_new.reshape(-1, 1, 2)\n",
        "\n",
        "    # ----- Object Detection with YOLOv5 -----\n",
        "    results = model.predict(frame_stabilized)\n",
        "    detections = results.xyxy[0]  # Bounding boxes for detected objects\n",
        "\n",
        "    # Loop through the detections and draw boxes and labels\n",
        "    for *box, conf, cls in detections:\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        confidence = float(conf)\n",
        "        class_id = int(cls)\n",
        "        label = model.model.names[class_id]  # Get the label for the detected class\n",
        "\n",
        "        # Draw bounding boxes and put the label with confidence score on the frame\n",
        "        cv2.rectangle(frame_stabilized, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        label_text = f\"{label}: {confidence:.2f}\"\n",
        "        cv2.putText(frame_stabilized, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "    # Display the frame with detected objects, motion tracking, and stabilization\n",
        "    cv2_imshow(frame_stabilized)\n",
        "\n",
        "    # Press 'q' to exit the video loop (you can skip this in Google Colab)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "OW2gHSerynh_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}